# -*- coding: utf-8 -*-
"""News.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15S5BkzdjDV2EJ5u6qclxsniQ3CnKMqi7
"""

pip install spacy

pip install nltk

import spacy
import nltk
from nltk.tokenize import word_tokenize

nltk.download('punkt_tab')

text = ("The investigation into the murder of Indore businessman Raja Raghuvanshi has intensified, "
        "with prime suspect Sonam Raghuvanshi and co-accused Raj Kushwaha presenting conflicting accounts during interrogation.")

print("NLTK Tokenization:")
nltk_tokens = word_tokenize(text)
print(nltk_tokens)

print("\nspaCy Tokenization:")
nlp = spacy.load("en_core_web_sm")
doc = nlp(text)
spacy_tokens = [token.text for token in doc]
print(spacy_tokens)

# prompt: tokenization using spacy

# If you haven't downloaded the English language model, do so by uncommenting the next line
# !python -m spacy download en_core_web_sm

import spacy

# Load the English language model
nlp = spacy.load("en_core_web_sm")

sentence = "The investigation into the murder of Indore businessman Raja Raghuvanshi has intensified with prime suspect Sonam Raghuvanshi and co-accused Raj Kushwaha presenting conflicting accounts during interrogation."

# Process the sentence with SpaCy
doc = nlp(sentence)

# Extract tokens
spacy_tokens = [token.text for token in doc]
spacy_tokens

# prompt: import lemmatization and do

nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()

# Lemmatize the tokens obtained from SpaCy processing
lemmatized_tokens = [lemmatizer.lemmatize(token) for token in spacy_tokens]
lemmatized_tokens

# prompt: import stop word removal and do

nltk.download('stopwords')
from nltk.corpus import stopwords

stop_words = set(stopwords.words('english'))

# Remove stop words from the list of tokens (using Spacy tokens here)
filtered_tokens = [token for token in spacy_tokens if token not in stop_words]
filtered_tokens